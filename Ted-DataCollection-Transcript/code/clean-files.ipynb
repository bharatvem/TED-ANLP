{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the correct path here & then create a folder with the name \"clean\" inside this folder\n",
    "#  \"clean\" folder will the folder with the cleaned file\n",
    "here = os.path.abspath(\"C:\\\\Users\\\\Beast\\\\Desktop\\\\Project_Run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the list of files that need to be processed\n",
    "file_list = []\n",
    "for file in os.listdir(here):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r\"[\\w(')]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank,stopwords\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read new stopwords and append to existing stopwords(sw) from nltk\n",
    "fpcsv = os.path.join(here,\"stop-word-list.csv\")\n",
    "f_sw = open(fpcsv,'r')\n",
    "cr = f_sw.read()\n",
    "new_sw = set(tokenizer.tokenize(cr) + list(sw))\n",
    "#print(new_sw) # new_sw are the new stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import hmm\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger = trainer.train_supervised(treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordlem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in file_list:\n",
    "    # set filepaths for read & write files\n",
    "    fpr = os.path.join(here,filename)\n",
    "    fpw = os.path.join(here,\"clean\",filename)\n",
    "    \n",
    "    # read one file and then convert to lower\n",
    "    text = open(fpr,'r')\n",
    "    process_text = text.read()\n",
    "    text.close()\n",
    "    process_text = process_text.lower()\n",
    "    \n",
    "    clean_file = open(fpw,\"w\") # open new file for cleaned data in new directory\n",
    "    \n",
    "    for sentn in sent_tokenize(process_text) :\n",
    "        sentn = re.sub(r'\\([^)]*\\)', '', sentn)\n",
    "        words = tokenizer.tokenize(sentn)\n",
    "        #print(tagger.tag(j)[0][0])\n",
    "        #x.write(str(tagger.tag(j)))\n",
    "        words = [x for x in words if x not in new_sw]\n",
    "        for w in words:\n",
    "            clean_file.write(str(wordlem.lemmatize(w)+' '))\n",
    "    \n",
    "    \n",
    "    clean_file.close()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t = txt.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#t = t.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HiddenMarkovModelTagger 46 states and 12408 output symbols>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk.corpus import wordnet\n",
    "#wordnet.synsets('dog')\n",
    "#wordnet.synsets('dog').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sss = re.sub(r'\\([^)]*\\)', '', sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r\"[\\w(')]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sdlk', 'kdsl', 'is', \"don't\", 'sdl', \"lsdkf'lllkjlkj\", 'jsdl', \"i'm\"]\n"
     ]
    }
   ],
   "source": [
    "#print(tokenizer.tokenize(sss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#z = tokenizer.tokenize(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bird'"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wordlem.lemmatize(\"bird\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
